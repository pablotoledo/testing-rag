{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talking with a Repository - Understanding RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Project is be able to talk with your own repository. This notebook will try to define a pipeline to work with the documentation. The main steps will be:\n",
    "\n",
    "- Prepare the environment\n",
    "- Get the repository\n",
    "- Do the Ingest step \n",
    "...\n",
    "\n",
    "This notebook is based in https://github.com/ZeusSama0001/RAG-chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment\n",
    "\n",
    "These are the main libraries used for each step in the Pipeline:\n",
    "\n",
    "`Get the repository`\n",
    "- GitPython \n",
    "\n",
    "`Ingest step`\n",
    "- The idea is use all-MiniLM-L6-v2 from HuggingFace as a model trained to obtain embeddings from texts and files.\n",
    "- Using LangChain for TextDecoder and store embeddings in a FAISS db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the git repository step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone locally a repository from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your branch is up to date with 'origin/main'.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# remove /repo directory if it exists using python\n",
    "if os.path.exists(\"repo\"):\n",
    "    shutil.rmtree(\"repo\")\n",
    "\n",
    "repo_path = os.getcwd() + \"/repo\"\n",
    "repo_url = \"https://github.com/pablotoledo/the-mergementor.git\"\n",
    "Repo.clone_from(repo_url, repo_path)\n",
    "\n",
    "repo = Repo(repo_path)\n",
    "assert not repo.bare\n",
    "\n",
    "# Move to the main branch\n",
    "repo.git.checkout('main')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the content and create the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a way to represent words in a vector space. The idea is to represent words in a way that similar words are close to each other in the vector space. This is useful for many NLP tasks, such as sentiment analysis, text classification, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "DATA_PATH = 'repo/'\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# Create vector database\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(DATA_PATH, silent_errors=True, loader_cls=TextLoader, exclude='**/.git/**')\n",
    "\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 3228.87it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 4815.50it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the repository about? and which llm pretrained model is being used?',\n",
       " 'result': 'The repository contains a pre-trained language model called CodeReviewer from Microsoft, licensed under MIT License. The model is trained on a large corpus of code and can be fine-tuned for various NLP tasks like code completion, code search, and code generation.',\n",
       " 'source_documents': [Document(page_content='---\\n\\nThis repository contains the CodeReviewer model from Microsoft, which is licensed under the MIT License. \\n\\nMicrosoft CodeReviewer model\\nhttps://huggingface.co/microsoft/codereviewer\\n\\nCopyright (c) Microsoft', metadata={'source': 'repo/LICENSE'}),\n",
       "  Document(page_content='MIT License\\n\\nCopyright (c) 2023 pablotoledo\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\n---', metadata={'source': 'repo/LICENSE'})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. You are a developer expert who guides wbout a repository.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "#Retrieval QA Chain\n",
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=db.as_retriever(search_kwargs={'k': 2}), return_source_documents=True, chain_type_kwargs={'prompt': prompt})\n",
    "    return qa_chain\n",
    "\n",
    "#Loading the model\n",
    "def load_llm():\n",
    "    llm = CTransformers(\n",
    "        model = \"TheBloke/Llama-2-7B-Chat-GGML\",\n",
    "        model_type=\"llama\",\n",
    "        max_new_tokens = 512,\n",
    "        temperature = 0.5\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "def qa_bot():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "    return qa\n",
    "\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response\n",
    "\n",
    "final_result(\"What is the repository about? and which llm pretrained model is being used?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py\n",
    "\n",
    "Knowing how RAG is working in this notebook the main.py is prepared to be used with Chainlit.\n",
    "\n",
    "```bash\n",
    "chainlit run main.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
